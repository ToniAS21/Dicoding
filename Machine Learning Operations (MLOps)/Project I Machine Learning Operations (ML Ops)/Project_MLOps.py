# -*- coding: utf-8 -*-
"""Project_MLOps.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C28tl1bfawNucI-xxqdTQUcet0qyZGjK

# Pengolahan Data

## A. Data Ingestion
"""

!pip install tfx tensorflow_model_analysis

"""### Libraries"""

import os
import pandas as pd
import tensorflow_model_analysis as tfma
from tfx.types import Channel
from tfx.dsl.components.common.resolver import Resolver
from tfx.types.standard_artifacts import Model, ModelBlessing
from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2
from tfx.components import Transform, Trainer, Tuner, Evaluator, Pusher
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy

"""### Set Variabel


"""

PIPELINE_NAME = "stock-pipeline"
SCHEMA_PIPELINE_NAME = "stock-tfdv-schema"

#Directory untuk menyimpan artifact yang akan dihasilkan
PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)

# Path to a SQLite DB file to use as an MLMD storage.
METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')

# Output directory where created models from the pipeline will be exported.
SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)

# from absl import logging
# logging.set_verbosity(logging.INFO)

!unzip "/content/stock_sentiment.zip"

DATA_ROOT = "/content/stock_sentiment"

interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)

"""### Proses Data Ingestion"""

output = example_gen_pb2.Output(
    split_config = example_gen_pb2.SplitConfig(splits=[
        example_gen_pb2.SplitConfig.Split(name="train", hash_buckets=8),
        example_gen_pb2.SplitConfig.Split(name="eval", hash_buckets=2)
    ])
)
example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)

interactive_context.run(example_gen)

"""> Kita membagi dataset menjadi training ("train) dan evaluasi ("eval). Dengan cara mengatur parameter output_config dengan rasio 8:2, pada paramater has_buckets.

## B. Data Validation

### 1. Membuat Summary Statistic
"""

# Menggunakana komponen StatisticGen()
statistics_gen = StatisticsGen(
    examples=example_gen.outputs["examples"]
)


interactive_context.run(statistics_gen)

interactive_context.show(statistics_gen.outputs['statistics'])

"""### 2. Membuat Data Schema"""

# Menggunakan komponen ShcemaGen()

schema_gen = SchemaGen(statistics=statistics_gen.outputs["statistics"]
)
interactive_context.run(schema_gen)

interactive_context.show(schema_gen.outputs["schema"])

"""> Artinya kita memiliki dua fitur, yaitu "headline" dengan tipe bytes dan "is_sarcastic" dengan tipe integer. Kedua fitur tersebut haruslah lengkap untuk setiap dataset.

### 3. Mengidentifikasi Anomali pada Dataset
"""

# Menggunakan komponen ExampleValidator()  ---- Memerlukan keluaran statistics_gen dan schema_gen

example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)
interactive_context.run(example_validator)

# Menampilkan hasil dari validasi
interactive_context.show(example_validator.outputs['anomalies'])

"""> Berdasarkan hasil validasi di atas, tidak terdapat anomali pada dataset yang kita gunakan pada latihan ini.

## C. Data Preprocessing
"""

# Mendefinisikan nama dari module
TRANSFORM_MODULE_FILE = "stock_transform.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRANSFORM_MODULE_FILE}
# import tensorflow as tf
# LABEL_KEY = "Sentiment"
# FEATURE_KEY = "Text"
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
#     
# def preprocessing_fn(inputs):
#     """
#     Preprocess input features into transformed features
#     
#     Args:
#         inputs: map from feature keys to raw features.
#     
#     Return:
#         outputs: map from feature keys to transformed features.    
#     """
#     
#     outputs = {}
#     
#     outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])
#     outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)
#     
#     return outputs

"""> Module ini memiliki dua function, yaitu 
- `transformed_name()` digunakan untuk mengubah nama fitur yang telah ditransform (dengan menambahkan "_xf). 
- `preprocessing_fn()` digunakan untuk mengubaha data dalam fitur "headline" ke dalam bentuk lowercase dan fitur "is_sarcastic" memiliki format (tf.int64).
"""

# Mendefinisikan Komponen Transform()

transform = Transform(
    examples = example_gen.outputs['examples'], # memasukan input dari komponen ExampleGen
    schema = schema_gen.outputs['schema'],  # memasukan input dari komponen ScehamaGen
    module_file = os.path.abspath(TRANSFORM_MODULE_FILE) # memasukan input dari module_file yang berisi preprocessing function 
)
interactive_context.run(transform)

"""> Terlihat bahwa komponen Transform akan menghasilkan beberapa output mulai dari **transfrom_graph**, **transformed_examples** (transformed data), dll. Keseluruhan output ini nantinya digunakan untuk melakukan preprocessing terhadap serving set. Hal ini untuk membantu memastikan preprocessing yang dilakukan pada fase training sama dengan fase deployment sehingga dapat mecegah feature skew.

# Pengembangan dan Validasi Model

## Pengembangan Model

Pada tahapan ini terdiri dari 2 bagian besar yaitu menemukan parameter terbaik (*best parameters*) alias melakukan tuning hyperparameter menggunakan komponen `Tuner()`. Setelah itu, kita akan menggunakan *best parameters* dalam proses train model menggunakan komponen `Trainer()`.

### 1. Tuning Parameters
"""

# Mendefinisikan nama module 
TUNER_MODULE_FILE = 'stock_tuner.py'

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TUNER_MODULE_FILE}
# import keras_tuner as kt
# import tensorflow as tf
# import tensorflow_transform as tft
# from typing import NamedTuple, Dict, Text, Any
# from keras_tuner.engine import base_tuner
# from tensorflow.keras import layers
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# 
# LABEL_KEY = "Sentiment"
# FEATURE_KEY = "Text"
# NUM_EPOCHS = 5
# 
# 
# # ------------- NamedTuple
# TunerFnResult = NamedTuple("TunerFnResult", [
#     ("tuner", base_tuner.BaseTuner),
#     ("fit_kwargs", Dict[Text, Any]),
# ])
# 
# #-------------- Callback
# stop_early = tf.keras.callbacks.EarlyStopping(
#     monitor="val_binary_accuracy",
#     min_delta=0,
#     patience=12,
#     verbose=0,
#     mode="auto",
#     baseline=None,
#     restore_best_weights=True
# )
# 
# 
# # ----------- Mengubah Nama Fitur
# def transformed_name(key):
#   """Renaming transformed features"""
#   return key + "_xf"
# 
# 
# # ----------- Memuat data dalam format TFRecord.
# def gzip_reader_fn(filenames):
#     return tf.data.TFRecordDataset(filenames, compression_type="GZIP")
# 
# 
# # ----------- Memuat transformed_feature dan membagi dalam beberapa batch
# def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64):
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy()
#     )
#     
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key=transformed_name(LABEL_KEY),
#     )
#     return dataset
# 
# 
# # ----------- Model Eksperimen
# def model_builder(hp, vectorizer_layer):
#     # Proses Eksperimen
#     num_hidden_layers = hp.Choice('num_hidden_layers', values=[1, 2])
#   
#     embed_dims = hp.Int('embed_dims', min_value=16, 
#                       max_value=128, step=16)
#   
#     lstm_units = hp.Int("lstm_units",  min_value=32, 
#                       max_value=256, step=16)
#   
#     dense_units = hp.Int('dense_units', min_value=64, 
#                        max_value=512, step=16)
#   
#     dropout_rate = hp.Float("dropout_rate", min_value=0.05,
#                           max_value=0.45, step=0.1)
#   
#     learning_rate = hp.Choice('learning_rate', values=[0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001])
# 
#     inputs = tf.keras.Input(
#         shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string
#     )
#     
#     x = vectorizer_layer(inputs)
#     x = layers.Embedding(input_dim=5000, output_dim=embed_dims)(x)
#     x = layers.Bidirectional(layers.LSTM(lstm_units))(x)
# 
#     for _ in range(num_hidden_layers):
#         x = layers.Dense(dense_units, activation=tf.nn.relu)(x)
#         x = layers.Dropout(dropout_rate)(x)
#     
#     outputs = layers.Dense(1, activation=tf.nn.sigmoid)(x)
#     
#     model = tf.keras.Model(inputs=inputs, outputs=outputs)
# 
#     model.compile(
#         optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
#         loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
#         metrics=["binary_accuracy"],
#     )
#     
#     return model
# 
# 
# # ----------- tuner_fn()
# def tuner_fn(fn_args: FnArgs):
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
#     
#     train_set = input_fn(
#         fn_args.train_files[0], tf_transform_output, NUM_EPOCHS
#     )
#     eval_set = input_fn(
#         fn_args.eval_files[0], tf_transform_output, NUM_EPOCHS
#     )
# 
#     vectorizer_dataset = train_set.map(
#         lambda f, l: f[transformed_name(FEATURE_KEY)]
#     )
# 
#     vectorizer_layer = layers.TextVectorization(
#         max_tokens=5000,
#         output_mode="int",
#         output_sequence_length=500,
#     )
#     vectorizer_layer.adapt(vectorizer_dataset)
#     
#     tuner = kt.Hyperband(
#         hypermodel=lambda hp: model_builder(hp, vectorizer_layer),
#         objective=kt.Objective('binary_accuracy', direction='max'),
#         max_epochs=NUM_EPOCHS,
#         factor=3,
#         directory=fn_args.working_dir,
#         project_name="kt_hyperband",
#     )
#     
#     return TunerFnResult(
#         tuner=tuner,
#         fit_kwargs={
#             "callbacks": [stop_early],
#             "x": train_set,
#             "validation_data": eval_set,
#             "steps_per_epoch": fn_args.train_steps,
#             "validation_steps": fn_args.eval_steps,
#         },
#     )

# Mendefinisikan komponen Tuner dan menjalankannya menggunakan interactive_context()

tuner = Tuner(
    module_file=os.path.abspath(TUNER_MODULE_FILE),
    examples=transform.outputs["transformed_examples"],
    transform_graph=transform.outputs["transform_graph"],
    schema=schema_gen.outputs["schema"],
    train_args=trainer_pb2.TrainArgs(splits=["train"], num_steps=80),
    eval_args=trainer_pb2.EvalArgs(splits=["eval"], num_steps=20),
)
interactive_context.run(tuner)

"""### 2. Training Model"""

TRAINER_MODULE_FILE = 'stock_trainer.py'

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRAINER_MODULE_FILE}
# import os
# import tensorflow as tf
# import tensorflow_transform as tft
# import tensorflow_hub as hub
# from tensorflow.keras import layers
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# LABEL_KEY = 'Sentiment'
# FEATURE_KEY = 'Text'
# 
# 
# # ----------- Mengubah Nama Fitur
# def transformed_name(key):
#   """Renaming transformed features"""
#   return key + "_xf"
# 
# 
# # ----------- Memuat data dalam format TFRecord.
# def gzip_reader_fn(filenames):
#     return tf.data.TFRecordDataset(filenames, compression_type="GZIP")
# 
# 
# # ----------- Memuat transformed_feature dan membagi dalam beberapa batch
# def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64):
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy()
#     )
#     
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key=transformed_name(LABEL_KEY),
#     )
#     return dataset
# 
# 
# # ---------- Arsitektur Model
# def model_builder(vectorizer_layer, hp):
#     inputs = tf.keras.Input(
#         shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string
#     )
# 
#     x = vectorizer_layer(inputs)
#     x = layers.Embedding(input_dim=5000, output_dim=hp["embed_dims"])(x)
#     x = layers.Bidirectional(layers.LSTM(hp["lstm_units"]))(x)
# 
#     for _ in range(hp["num_hidden_layers"]):
#         x = layers.Dense(hp["dense_units"], activation=tf.nn.relu)(x)
#         x = layers.Dropout(hp["dropout_rate"])(x)
# 
#     outputs = layers.Dense(1, activation=tf.nn.sigmoid)(x)
#     
#     model = tf.keras.Model(inputs=inputs, outputs = outputs)
# 
#     model.compile(
#         optimizer=tf.keras.optimizers.Adam(learning_rate=hp["learning_rate"]),
#         loss=tf.keras.losses.BinaryCrossentropy(),
#         metrics=[tf.keras.metrics.BinaryAccuracy()],
#     )
#     model.summary()
#     
#     return model
# 
# 
# # ---------- Menjalankan tahapan preprocessing data pada raw request data.
# def _get_serve_tf_examples_fn(model, tf_transform_output):
#     model.tft_layer = tf_transform_output.transform_features_layer()
#     
#     @tf.function
#     def serve_tf_examples_fn(serialized_tf_examples):
#         feature_spec = tf_transform_output.raw_feature_spec()
#         feature_spec.pop(LABEL_KEY)
#         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
#         transformed_features = model.tft_layer(parsed_features)
# 
#         # get predictions using the transformed features
#         return model(transformed_features)
#         
#     return serve_tf_examples_fn
# 
# 
# # ----------- Run Training
# def run_fn(fn_args: FnArgs):
#     hp = fn_args.hyperparameters["values"]
#     log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), "logs")
#     
#     # Define Tensorboard
#     tensorboard_callback = tf.keras.callbacks.TensorBoard(
#         log_dir=log_dir, update_freq="batch")
#     
#     # Define Callback
#     early_stop = tf.keras.callbacks.EarlyStopping(monitor="val_binary_accuracy",
#                                                   min_delta=0,
#                                                   patience=12,
#                                                   verbose=0,
#                                                   mode="auto",
#                                                   baseline=None,
#                                                   restore_best_weights=True)
#     
#     # Define Model Check Poin Callbacks
#     model_checkpoint = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, 
#                                                           monitor='val_binary_accuracy', 
#                                                           mode='max', 
#                                                           verbose=1, 
#                                                           save_best_only=True)
#     
#     # Load the transform output
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     # Create batches of data
#     train_set = input_fn(fn_args.train_files, 
#                          tf_transform_output, 
#                          hp['tuner/epochs'])
#     
#     eval_set = input_fn(fn_args.eval_files, 
#                         tf_transform_output, 
#                         hp['tuner/epochs'])
#     
#     vectorizer_dataset = train_set.map(
#         lambda f, l: f[transformed_name(FEATURE_KEY)])
#     
#     vectorizer_layer = layers.TextVectorization(
#         max_tokens=5000,
#         output_mode='int',
#         output_sequence_length=500
#     )
#     vectorizer_layer.adapt(vectorizer_dataset)
# 
#     # Build the model
#     model = model_builder(vectorizer_layer, hp)
#     
#     # Train the model
#     model.fit(x = train_set,
#               steps_per_epoch=fn_args.train_steps,
#               validation_data=eval_set,
#               validation_steps=fn_args.eval_steps,
#               callbacks=[tensorboard_callback, 
#                          early_stop, 
#                          model_checkpoint],
#               epochs=hp['tuner/epochs'],
#               verbose=1
#               )
#     
#     signatures = {
#         'serving_default': _get_serve_tf_examples_fn(model, 
#                                                     tf_transform_output).get_concrete_function(tf.TensorSpec(shape=[None],
#                                                                                                              dtype=tf.string,
#                                                                                                              name='examples'))
#                   }
#     model.save(fn_args.serving_model_dir, 
#                save_format='tf', 
#                signatures=signatures)

# Mendefinisikan Komponen `Trainer()`

trainer = Trainer(
    module_file=os.path.abspath(TRAINER_MODULE_FILE),
    examples = transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    hyperparameters=tuner.outputs['best_hyperparameters'],
    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=80),
    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=20)
)
interactive_context.run(trainer)

"""## Analisis dan Validasi Model

### 1. Komponen Resolver

Untuk menyediakan sebuah baseline model dan ingin memilih lebih dari satu versi model dan membandingkan dua buah versi model yang berbeda.
"""

model_resolver = Resolver(
    strategy_class = LatestBlessedModelStrategy,
    model = Channel(type=Model),
    model_blessing = Channel(type=ModelBlessing)
).with_id('Latest_blessed_model_resolver')

interactive_context.run(model_resolver)

"""### 2. Komponen Evaluator
Membuat beberapa konfigurasi untuk mengevaluasi model dengan menggunakan library TFMA.
"""

eval_config = tfma.EvalConfig(
    model_specs=[tfma.ModelSpec(label_key='Sentiment')],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(metrics=[            
            tfma.MetricConfig(class_name='ExampleCount'),
            tfma.MetricConfig(class_name='AUC'),
            tfma.MetricConfig(class_name='FalsePositives'),
            tfma.MetricConfig(class_name='TruePositives'),
            tfma.MetricConfig(class_name='FalseNegatives'),
            tfma.MetricConfig(class_name='TrueNegatives'),
            tfma.MetricConfig(class_name='BinaryAccuracy',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(
                        lower_bound={'value':0.5}),
                    change_threshold=tfma.GenericChangeThreshold(
                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                        absolute={'value':0.0001})
                    )
            )
        ])
    ]
 
)

evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)

interactive_context.run(evaluator)

"""Visualisasi hasil evalluasi menggunakan library TFMA"""

# Visualize the evaluation results
eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(
    tfma_result
)

"""# Penerapan Model (Deployment)

## 1. Menambahkan Komponen Pusher
"""

pusher = Pusher(
    model=trainer.outputs['model'],
    model_blessing=evaluator.outputs['blessing'],
    push_destination=pusher_pb2.PushDestination(
        filesystem=pusher_pb2.PushDestination.Filesystem(
            base_directory='serving_model_dir/stock-sentiment-detection-model'))
)

interactive_context.run(pusher)

"""## 2. Compress folder"""

!zip -r /content/pipelines.zip /content/pipelines

# Untuk donwload folder serving_model_dir
from google.colab import files
!zip -r /content/serving_model_dir.zip /content/serving_model_dir
files.download('/content/pipelines.zip')

!pip freeze >> requirements.txt